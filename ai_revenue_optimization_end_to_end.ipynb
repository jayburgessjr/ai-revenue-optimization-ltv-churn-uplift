{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1554927a",
   "metadata": {},
   "source": [
    "\n",
    "# AI-Driven Revenue Optimization: LTV, Churn & Uplift Modeling (End-to-End)\n",
    "\n",
    "**Author:** James \"Jay\" Burgess  \n",
    "**Role:** Data & AI Engineer · Applied Data Scientist · Revenue Architect\n",
    "\n",
    "This notebook is designed as a *portfolio-grade* showcase.\n",
    "\n",
    "It simulates a realistic B2C/B2B funnel and walks through:\n",
    "\n",
    "1. Synthetic but structured **Lakehouse-style** data generation (customers, marketing, product usage, revenue).\n",
    "2. Feature engineering for:\n",
    "   - Customer Lifetime Value (LTV)\n",
    "   - Churn risk\n",
    "   - Marketing responsiveness\n",
    "3. Training:\n",
    "   - A **churn model** (classification)\n",
    "   - An **LTV model** (regression)\n",
    "   - A simple **uplift / treatment effect** model (who should get offers)\n",
    "4. Evaluation with real metrics (AUC, RMSE, calibration checks).\n",
    "5. **Explainability & decision intelligence**:\n",
    "   - Feature importance\n",
    "   - Policy simulation: which customers to target to maximize incremental revenue.\n",
    "\n",
    "This is how I think about AI/ML: not as a toy model, but as a decision system wired to revenue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2695bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d7847",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Generate Synthetic Customer & Funnel Data\n",
    "\n",
    "We simulate:\n",
    "\n",
    "- Acquisition channels & spend efficiency\n",
    "- Engagement behavior (sessions, product usage, support touches)\n",
    "- An experimental **offer** (treatment vs control)\n",
    "- Resulting revenue over an observation window\n",
    "- Churn (binary) as *no revenue after a cutoff*\n",
    "\n",
    "The goal: create a dataset that *behaves like* what you'd see in a real growth / RevOps environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_customers = 20000\n",
    "\n",
    "# Acquisition channels\n",
    "channels = [\"paid_search\", \"paid_social\", \"organic\", \"affiliate\", \"direct\"]\n",
    "channel_probs = [0.25, 0.25, 0.2, 0.15, 0.15]\n",
    "\n",
    "acq_channel = np.random.choice(channels, size=n_customers, p=channel_probs)\n",
    "\n",
    "# Base propensities by channel\n",
    "channel_base_value = {\n",
    "    \"paid_search\": 1.1,\n",
    "    \"paid_social\": 0.9,\n",
    "    \"organic\": 1.3,\n",
    "    \"affiliate\": 1.0,\n",
    "    \"direct\": 1.4,\n",
    "}\n",
    "\n",
    "# Customer attributes\n",
    "tenure_months = np.random.gamma(shape=3, scale=4, size=n_customers)  # >0\n",
    "tenure_months = np.clip(tenure_months, 1, 48)\n",
    "\n",
    "monthly_visits = np.random.poisson(lam=4, size=n_customers) + np.random.binomial(4, 0.3, size=n_customers)\n",
    "monthly_visits = np.clip(monthly_visits, 0, None)\n",
    "\n",
    "product_events = np.random.poisson(lam=8, size=n_customers) + (monthly_visits * np.random.uniform(0.5, 1.5, n_customers))\n",
    "product_events = np.clip(product_events, 0, None)\n",
    "\n",
    "support_tickets = np.random.poisson(lam=0.3, size=n_customers)\n",
    "region = np.random.choice([\"NA\", \"EU\", \"LATAM\", \"APAC\"], size=n_customers, p=[0.4, 0.25, 0.2, 0.15])\n",
    "\n",
    "# Experimental treatment: some customers receive an offer\n",
    "treatment = np.random.binomial(1, 0.5, size=n_customers)\n",
    "\n",
    "# Latent \"affinity\" score\n",
    "base_affinity = (\n",
    "    0.05 * tenure_months\n",
    "    + 0.02 * monthly_visits\n",
    "    + 0.015 * product_events\n",
    "    - 0.1 * support_tickets\n",
    "    + np.array([channel_base_value[c] for c in acq_channel])\n",
    "    + np.random.normal(0, 1, n_customers)\n",
    ")\n",
    "\n",
    "# Channel multipliers for revenue\n",
    "channel_multiplier = {\n",
    "    \"paid_search\": 1.0,\n",
    "    \"paid_social\": 0.8,\n",
    "    \"organic\": 1.2,\n",
    "    \"affiliate\": 0.9,\n",
    "    \"direct\": 1.3,\n",
    "}\n",
    "\n",
    "# Treatment effect: only some segments lift\n",
    "treatment_effect = (\n",
    "    0.25 * (np.isin(acq_channel, [\"paid_search\", \"paid_social\"]).astype(int))\n",
    "    + 0.15 * (monthly_visits > 4)\n",
    ")\n",
    "\n",
    "# Baseline LTV (over 6-12 months)\n",
    "baseline_ltv = np.exp(2.2 + 0.12 * base_affinity / np.std(base_affinity))\n",
    "baseline_ltv = baseline_ltv / 50.0  # scale down\n",
    "\n",
    "# Apply channel multiplier\n",
    "baseline_ltv *= np.array([channel_multiplier[c] for c in acq_channel])\n",
    "\n",
    "# Apply treatment uplift on a multiplicative scale\n",
    "ltv_with_treatment = baseline_ltv * (1 + treatment * treatment_effect)\n",
    "\n",
    "# Introduce noise\n",
    "observed_ltv = ltv_with_treatment * np.exp(np.random.normal(0, 0.35, n_customers))\n",
    "\n",
    "# Cap LTV\n",
    "observed_ltv = np.clip(observed_ltv, 0, 5000)\n",
    "\n",
    "# Define churn: no significant revenue (e.g., LTV < threshold)\n",
    "churn = (observed_ltv < 150).astype(int)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"customer_id\": np.arange(1, n_customers + 1),\n",
    "    \"acq_channel\": acq_channel,\n",
    "    \"tenure_months\": tenure_months.round(1),\n",
    "    \"monthly_visits\": monthly_visits,\n",
    "    \"product_events\": product_events,\n",
    "    \"support_tickets\": support_tickets,\n",
    "    \"region\": region,\n",
    "    \"treatment_offer\": treatment,\n",
    "    \"observed_ltv\": observed_ltv.round(2),\n",
    "    \"churned\": churn,\n",
    "})\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4473f8",
   "metadata": {},
   "source": [
    "\n",
    "### Quick Sanity Check\n",
    "\n",
    "Check distribution of LTV and churn to ensure this \"feels\" like a real-world dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ffc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(data.describe(include='all').T)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(data[\"observed_ltv\"], bins=50)\n",
    "plt.title(\"Observed LTV Distribution\")\n",
    "plt.xlabel(\"LTV\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "churn_rate = data[\"churned\"].mean()\n",
    "print(f\"Churn rate: {churn_rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0afce4",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Feature Engineering\n",
    "\n",
    "We:\n",
    "- One-hot encode categorical variables\n",
    "- Create interaction-style features (e.g., intensity / tenure)\n",
    "- Keep the pipeline explicit so it reads like production code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = data.copy()\n",
    "\n",
    "# Basic derived features\n",
    "df[\"events_per_visit\"] = np.where(df[\"monthly_visits\"] > 0,\n",
    "                                  df[\"product_events\"] / df[\"monthly_visits\"],\n",
    "                                  0)\n",
    "df[\"support_intensity\"] = df[\"support_tickets\"] / (df[\"tenure_months\"] + 1)\n",
    "\n",
    "# One-hot encode\n",
    "cat_cols = [\"acq_channel\", \"region\"]\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "feature_cols = [c for c in df_encoded.columns\n",
    "                if c not in [\"customer_id\", \"observed_ltv\", \"churned\"]]\n",
    "\n",
    "X = df_encoded[feature_cols]\n",
    "y_churn = df_encoded[\"churned\"]\n",
    "y_ltv = df_encoded[\"observed_ltv\"]\n",
    "\n",
    "X.shape, y_churn.shape, y_ltv.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7e7dd",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Train/Test Split\n",
    "\n",
    "We keep the split simple and reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b349e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_churn_train, y_churn_test, y_ltv_train, y_ltv_test = train_test_split(\n",
    "    X, y_churn, y_ltv, test_size=0.25, random_state=42, stratify=y_churn\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02403d",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Churn Model (Classification)\n",
    "\n",
    "We start with a Gradient Boosting Classifier as a strong baseline.\n",
    "\n",
    "We care about:\n",
    "- ROC-AUC\n",
    "- Rank ordering (who is likely to churn)\n",
    "- Business interpretation: target high-risk customers with retention actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9795e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "churn_model = GradientBoostingClassifier(random_state=42)\n",
    "churn_model.fit(X_train, y_churn_train)\n",
    "\n",
    "churn_proba_test = churn_model.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_churn_test, churn_proba_test)\n",
    "print(f\"Churn ROC-AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr, tpr, _ = roc_curve(y_churn_test, churn_proba_test)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Churn Model ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e4b57",
   "metadata": {},
   "source": [
    "\n",
    "## 5. LTV Model (Regression)\n",
    "\n",
    "We model *log(LTV + 1)* for stability.\n",
    "\n",
    "This supports:\n",
    "- Segmentation by value\n",
    "- Forecasting revenue impact of cohorts\n",
    "- Feeding downstream bidding / targeting / prioritization systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9221f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_ltv_train_log = np.log1p(y_ltv_train)\n",
    "y_ltv_test_log = np.log1p(y_ltv_test)\n",
    "\n",
    "ltv_model = GradientBoostingRegressor(random_state=42)\n",
    "ltv_model.fit(X_train, y_ltv_train_log)\n",
    "\n",
    "y_ltv_pred_log = ltv_model.predict(X_test)\n",
    "y_ltv_pred = np.expm1(y_ltv_pred_log)\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(y_ltv_test, y_ltv_pred))\n",
    "print(f\"LTV RMSE: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9754a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(y_ltv_test, y_ltv_pred, alpha=0.3)\n",
    "plt.xlabel(\"Actual LTV\")\n",
    "plt.ylabel(\"Predicted LTV\")\n",
    "plt.title(\"LTV: Actual vs Predicted\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83787fa3",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Uplift Modeling (Who Should Get the Offer?)\n",
    "\n",
    "We approximate uplift in a **two-model** fashion:\n",
    "\n",
    "1. Train separate models:\n",
    "   - on treated customers\n",
    "   - on control customers\n",
    "\n",
    "2. For each customer, estimate:\n",
    "   - predicted LTV if treated\n",
    "   - predicted LTV if not treated\n",
    "\n",
    "3. Uplift = LTV_treated - LTV_control\n",
    "\n",
    "In production, you'd likely use:\n",
    "- Causal forests\n",
    "- Meta-learners (T-learner / X-learner)\n",
    "- Or library support for uplift models\n",
    "\n",
    "Here we show the concept in a compact, readable way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a386b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "treated_mask = (df_encoded[\"treatment_offer\"] == 1)\n",
    "control_mask = (df_encoded[\"treatment_offer\"] == 0)\n",
    "\n",
    "X_treated = X[treated_mask]\n",
    "y_treated = np.log1p(y_ltv[treated_mask])\n",
    "\n",
    "X_control = X[control_mask]\n",
    "y_control = np.log1p(y_ltv[control_mask])\n",
    "\n",
    "model_treated = RandomForestClassifier(n_estimators=80, random_state=42)\n",
    "model_control = RandomForestClassifier(n_estimators=80, random_state=42)\n",
    "\n",
    "# For simplicity in this demo, model uplift on high-vs-low value\n",
    "high_value_treated = (y_ltv[treated_mask] > y_ltv.median()).astype(int)\n",
    "high_value_control = (y_ltv[control_mask] > y_ltv.median()).astype(int)\n",
    "\n",
    "model_treated.fit(X_treated, high_value_treated)\n",
    "model_control.fit(X_control, high_value_control)\n",
    "\n",
    "# Estimate uplift on full population\n",
    "proba_treated = model_treated.predict_proba(X)[:, 1]\n",
    "proba_control = model_control.predict_proba(X)[:, 1]\n",
    "uplift_score = proba_treated - proba_control\n",
    "\n",
    "df_uplift = df_encoded.copy()\n",
    "df_uplift[\"uplift_score\"] = uplift_score\n",
    "\n",
    "df_uplift[[\"customer_id\", \"uplift_score\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3212a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# How much incremental value if we target top 20% uplift?\n",
    "\n",
    "top_k = int(0.2 * len(df_uplift))\n",
    "top_segment = df_uplift.sort_values(\"uplift_score\", ascending=False).head(top_k)\n",
    "bottom_segment = df_uplift.sort_values(\"uplift_score\", ascending=True).head(top_k)\n",
    "\n",
    "avg_ltv_top = data.loc[top_segment.index, \"observed_ltv\"].mean()\n",
    "avg_ltv_bottom = data.loc[bottom_segment.index, \"observed_ltv\"].mean()\n",
    "\n",
    "print(f\"Avg LTV (top 20% predicted uplift):   {avg_ltv_top:8.2f}\")\n",
    "print(f\"Avg LTV (bottom 20% predicted uplift): {avg_ltv_bottom:8.2f}\")\n",
    "print(f\"Lift ratio: {avg_ltv_top / max(avg_ltv_bottom,1e-6):.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd6d6a",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Model Explainability (Feature Importance)\n",
    "\n",
    "We inspect which signals the models use.\n",
    "\n",
    "The point isn’t just accuracy — it’s **governance**:\n",
    "stakeholders must see *why* the system is making decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0328b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_feature_importance(model, feature_names, top_n=15, title=\"Feature Importance\"):\n",
    "    importances = model.feature_importances_\n",
    "    idx = np.argsort(importances)[::-1][:top_n]\n",
    "    plt.figure(figsize=(6, max(4, top_n * 0.25)))\n",
    "    plt.barh(range(len(idx)), importances[idx][::-1])\n",
    "    plt.yticks(range(len(idx)), [feature_names[i] for i in idx][::-1])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(churn_model, feature_cols, top_n=15, title=\"Churn Model Feature Importance\")\n",
    "plot_feature_importance(ltv_model, feature_cols, top_n=15, title=\"LTV Model Feature Importance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762ccb6",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Framing This for Recruiters & Hiring Managers\n",
    "\n",
    "This notebook is intentionally designed to read like **production thinking**:\n",
    "\n",
    "- Starts from a **business problem**: who to acquire, retain, and target to maximize revenue.\n",
    "- Simulates realistic **multi-source data**: channels, behavior, support, offers, outcomes.\n",
    "- Builds:\n",
    "  - A churn model for retention plays.\n",
    "  - An LTV model for value-based prioritization.\n",
    "  - An uplift-style model for targeted incentives.\n",
    "- Includes:\n",
    "  - Performance metrics (ROC-AUC, RMSE).\n",
    "  - Sanity checks & visual diagnostics.\n",
    "  - Interpretability via feature importance.\n",
    "  - A simple **policy simulation** (top uplift vs bottom uplift).\n",
    "\n",
    "Use this in your portfolio as:\n",
    "\n",
    "> \"AI-Driven Revenue Optimization in a Lakehouse Context: from synthetic data design to decision-ready models.\"\n",
    "\n",
    "In a Databricks environment, this naturally maps to:\n",
    "- Delta tables instead of local DataFrames\n",
    "- Unity Catalog for governance\n",
    "- MLflow for experiment tracking\n",
    "- Jobs / DLT for orchestration\n",
    "\n",
    "The structure here is the important part: it proves I know how to turn models into levers.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
